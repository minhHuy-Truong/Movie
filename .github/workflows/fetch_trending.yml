name: API and Web Scrape Automation
on:
  schedule:
    - cron: "0 */2 * * *"  # Ch·∫°y m·ªói 2 gi·ªù
  workflow_dispatch:  # Cho ph√©p ch·∫°y th·ªß c√¥ng
permissions:
  contents: write   # üëà Th√™m quy·ªÅn ghi v√†o repo ƒë·ªÉ c√≥ th·ªÉ push
jobs:
  update_data:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repo
      uses: actions/checkout@v4
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        python -m playwright install --with-deps chromium
    - name: Run crawler
      env:
        TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
        OMDB_API_KEY: ${{ secrets.OMDB_API_KEY }}
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
        X_API_KEY: ${{ secrets.X_API_KEY }}
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
      run: python upload_data.py
    - name: Commit and push results
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git add top10_trending_full.json top10_trending.csv
        git diff --quiet && git diff --staged --quiet || git commit -m "Update trending data - $(date +'%Y-%m-%d %H:%M:%S')"
        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
